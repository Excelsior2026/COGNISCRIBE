version: '3.8'

services:
  # Ollama service for LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: cogniscribe-ollama
    volumes:
      - ollama_data:/root/.ollama
      - ./scripts/init-ollama.sh:/init-ollama.sh
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    # Uncomment below for GPU support (requires nvidia-docker)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    restart: unless-stopped
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        ollama serve &
        sleep 10
        ollama pull llama3.1:8b
        wait

  # Main API service
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: cogniscribe-api
    ports:
      - "8080:8080"
    environment:
      # Ollama configuration
      OLLAMA_HOST: ollama
      OLLAMA_PORT: 11434
      OLLAMA_MODEL: llama3.1:8b
      OLLAMA_TIMEOUT: 300
      
      # Whisper configuration
      WHISPER_MODEL: base
      USE_GPU: "false"
      
      # File configuration
      MAX_FILE_SIZE_MB: 1000
      AUDIO_RETENTION_DAYS: 7
      
      # API configuration
      CLINISCRIBE_AUTH_ENABLED: "false"
      LOG_LEVEL: INFO
      
      # CORS configuration
      CORS_ALLOW_ORIGINS: http://localhost:5173,http://localhost:3000
    volumes:
      - audio_storage:/app/audio_storage
      - temp_audio:/app/temp_processed
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Frontend service
  frontend:
    build:
      context: ./client/web-react
      args:
        VITE_API_URL: http://localhost:8080
    container_name: cogniscribe-frontend
    ports:
      - "5173:80"
    depends_on:
      api:
        condition: service_healthy
    restart: unless-stopped

volumes:
  # Persistent storage for Ollama models (can be large ~4-10GB)
  ollama_data:
    driver: local
  
  # Persistent storage for uploaded audio files
  audio_storage:
    driver: local
  
  # Temporary audio processing directory
  temp_audio:
    driver: local

networks:
  default:
    name: cogniscribe-network
